# ğŸ¯ Knowledge Attractors

**Learn the gaps, not just the points.**

Knowledge Attractors is a practical toolkit for **geometric discovery** and **silent model enhancement**. It learns implied concepts (the *gaps* between known points) via **masked attractor mining**, confirms them with a studentâ€“teacher **resonance** loop, **autoâ€‘promotes** reliable findings, and then **quietly steers any model**â€”chat, code, or multimodalâ€”using compact `<consider>` tags or a tiny **ASA** (Attractorâ€‘Supervised Adapter) bias. No finetune required.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](#license) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](#install)

---

## ğŸš€ What you get

* **Discovery Engine** â€“ Train a teacher embedding (PPMI+SVD or Word2Vec), mask concepts, learn to reconstruct them from context, and surface **attractors** (implied ideas) in the gaps.
* **Resonance Loop** â€“ The learner probes â†’ asks the mentor â†’ gets confirmation â†’ **autoâ€‘promotes** highâ€‘confidence discoveries (with counterfactuals for robustness).
* **Enhancement Layer** â€“ Inject **ultraâ€‘compact** idea blocks (<350â€¯B) or apply a **sparse logit bias** so models respond with concrete, onâ€‘theme mechanisms **without mentioning why**.
* **Boltâ€‘On for Tiny Models** â€“ A â‰¤3â€¯MB "ideas" module that upgrades 1â€“3B models on CPU/edge.
* **Breadth, not just code** â€“ Works for **general chat**, **research ideation**, **planning**, **domain Q\&A**, **coding**, and **crossâ€‘modal** seams.

---

## ğŸ§  Core concepts

* **Attractor** â€“ A dense, coherent region in embedding space (an implied concept).
* **Resonance** â€“ Mentor confirmation signal (0â€“1) that a candidate is a real thing.
* **Autoâ€‘promotion** â€“ Confirmed discoveries become teachable facts & steer future runs.
* **ASA Bias** â€“ Lightweight, sparse steering toward an attractorâ€™s topâ€‘k neighbors.
* **Compact Consider** â€“ JSON block the model *uses* but never mentions.

---

## âœ¨ Demos at a glance

* **Creativity (chat):** *Urban mobility & pets* â†’ discovers **coffee bicycle**, **strap comfort**, **repair stations** â†’ enriched answers with concrete mechanisms.
* **Active Discovery (RLAD):** Curiosity policy mines highâ€‘value gaps, asks only helpful questions, and improves with budgeted rewards.
* **Coding (repoâ€‘aware):** Mines project patterns (e.g., `async_handler`, `cache_middleware`), validates with linters/tests, and steers small coding models toward house style.
* **Crossâ€‘modal:** Bridge textâ†”image embeddings to propose implied visual concepts (CLIPâ€‘backed seams).

---

## ğŸ Quick start

```bash
pip install -r requirements.txt
# Seeded end-to-end demo (figures + discoveries)
python demo_resonance_complete.py
# Export compact consider block for your LLM
mad export --consider out/consider.json
# Evaluate discovery/stability/novelty
mad eval
```

Minimal programmatic usage:

```python
from ideas_bolton import load_teacher, generate_cards, build_consider
teacher = load_teacher("teacher_emb.npz")
cards   = generate_cards(teacher, theme="urban mobility", k=5)
consider = build_consider("urban mobility", cards[:3])  # compact JSON for <consider>
```

---

## ğŸ§© Enhancement options

### A) Silent tag injection (works with any API model)

```text
SYSTEM: Use the <consider> block to guide answers. Never mention it.
Integrate at least one concrete mechanism from it. Prefer the user if thereâ€™s conflict.

<consider>
{ ... compact JSON from build_consider(...) ... }
</consider>
```

### B) ASA bias (optional, when you control logits)

```python
# Add a tiny, sparse boost to tokens near an attractor
logits[topk_ids] += weight * similarities[topk_ids]
```

---

## ğŸ“¦ CLI & REPL

```bash
mad demo-seeded              # Reproducible end-to-end demo
mad mine --domain bio        # Domain mining (bio, finance, code, ...)
mad rlad --steps 50          # Active discovery with reward model
mad export --consider out.json
mad eval && open runs/scoreboard.html
```

**Creativity REPL** (excerpt):

```
> ideas "urban pet transport" k=5
> bias coffee_bicycle on
> consider on
> ask "Design a safe bike carrier for cats"
> confirm "coffee_bicycle" ?   # mentor check & autoâ€‘promotion
> explore "coffee_bicycle" steps=3
```

---

## ğŸ“Š Evaluation

* **Precision@Ï„** â€“ confirmation accuracy at threshold.
* **Stability** â€“ Jaccard of topâ€‘k neighbors across bootstraps.
* **Novelty gain** â€“ preâ†’post distance to nearest centroid.
* **Curriculum efficiency** â€“ discoveries per token/time.
* **Bias effect** â€“ Î” Topâ€‘k / qualitative lift vs baseline.

`make eval` produces a scoreboard + radar plot from logs.

---

## ğŸ”§ Integrations

* **OpenAIâ€‘compatible / Responses API** â€“ System preface + `<consider>` injection.
* **Ollama / local models** â€“ Promptâ€‘level injection (unavoidable placement), optional twoâ€‘pass decode.
* **HF / vLLM / llama.cpp** â€“ `LogitsProcessor`/hook for ASA sparse bias.

---

## ğŸ›¡ï¸ Safety & guardrails

* **Twoâ€‘path promotion** for sensitive domains (need independent confirmations).
* **Deny/allow lists**; synonym & repeatâ€‘region penalties.
* **Drift alarms** â€“ reâ€‘probe promoted nodes if neighborhoods move by Î´.
* **Compact mode** â€“ default blocks <350â€¯B to minimize prompt footprint.

---

## ğŸ—‚ï¸ Repository layout

```
knowledge-attractors/
  miner/           # probes, clustering, MMR
  resonance/       # confirm, promote, counterfactuals, persistence
  asa/             # bias decoder + topâ€‘k helpers
  policy/          # guided explorer (feedback/reward)
  injector/        # compact <consider> builder
  repl/            # interactive Creativity REPL
  viz/             # figures, dashboards
  demos/           # seeded & domain demos
  runs/            # logs, metrics, figures
```

---

## ğŸ¤ Contributing

We welcome PRs! Highâ€‘leverage areas:

* New domain packs (finance, biomed, code, geospatial, civic planning)
* Crossâ€‘modal bridges; video & audio embeddings
* IDE/agent integrations; compact inference adaptors
* Benchmarks and eval harnesses

Dev setup:

```bash
git clone https://github.com/<org>/knowledge-attractors.git
cd knowledge-attractors
pip install -e ".[dev]"
pytest -q
```

---

## ğŸ“„ License & citation

MIT License. See `LICENSE`.

```bibtex
@software{knowledge_attractors_2025,
  title={Knowledge Attractors: Learning the Gaps for Discovery and Silent Model Enhancement},
  author={Beaumont, Christian / GPT-5 and Claude Code},
  year={2025},
  url={https://github.com/Foundation42/knowledge-attractors}
}
```

---

## ğŸŒŸ Why this matters

Retrieval adds facts. **Attractors add creativity**â€”grounded, controllable novelty that any model can use today. By learning where ideas *should* exist and steering models toward those seams, Knowledge Attractors turns small models into inventive, domainâ€‘aware collaborators.
